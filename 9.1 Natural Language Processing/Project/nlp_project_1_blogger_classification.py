# -*- coding: utf-8 -*-
"""NLP project-1 - Blogger classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jzv0LI7QkfUDUoZCD989hOxaFOfGlgXn

# DOMAIN: Digital content management
## CONTEXT: 
Classification is probably the most popular task that you would deal with in real life. Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the information about the writer without knowing about him/her. We are going to create a classifier that predicts multiple features of the author of a given text. We have designed it as a Multi label classification problem.

### DATA DESCRIPTION: 
Over 600,000 posts from more than 19 thousand bloggers The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person. Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger’s self-provided gender, age, industry, and astrological sign. (All are labelled for gender and age but for many, industry and/or sign is marked as unknown.) All bloggers included in the corpus fall into one of three age groups:

• 8240 "10s" blogs (ages 13-17),

• 8086 "20s" blogs(ages 23-27) and

• 2994 "30s" blogs (ages 33-47)

For each age group, there is an equal number of male and female bloggers.
Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions.
Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label url
link. Link to dataset: https://www.kaggle.com/rtatman/blog-authorship-corpus

## PROJECT OBJECTIVE: 
The need is to build a NLP classifier which can use input text parameters to determine the label/s of of the blog.
## Steps and tasks: [ Total Score: 20 points]
1. Import and analyse the data set.
2. Perform data pre-processing on the data:

*   Data cleansing by removing unwanted characters, spaces, stop words etc. Convert text to lowercase.
*   Target/label merger and transformation
*   Train and test split
*   Vectorisation, etc.

3. Design, train, tune and test the best text classifier.
4. Display and explain detail the classification report
5. Print the true vs predicted labels for any 5 entries from the dataset.

* Hint: The aim here Is to import the text, process it such a way that it can be taken as an inout to the ML/NN classifiers. Be analytical and experimental here in trying new approaches to design the best model
"""

import warnings
warnings.filterwarnings('ignore')

!find / -iname 'libdevice'
!find / -iname 'libnvvm.so'

import os
os.environ['NUMBAPRO_LIBDEVICE'] = "/usr/local/cuda-11.0/nvvm/libdevice"
os.environ['NUMBAPRO_NVVM'] = "/usr/local/cuda-11.0/nvvm/lib64/libnvvm.so"

from numba import vectorize 
from numba import jit, cuda

import tensorflow as tf
# Enable GPU in the Colab settings for running the code faster
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import scipy as sp
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

import textblob as TextBlob
import spacy
import wordcloud as WordCloud
from nltk.stem.snowball import SnowballStemmer

"""# 1. Import and analyse the data set.

"""

from google.colab import drive
drive.mount('/content/drive')

project_path = '/content/drive/MyDrive/Colab/NLP/Project1/'

df = pd.read_csv(project_path + 'blogtext.csv', index_col=False)
df.head()

df.shape

df.dtypes

# Check for missing values
df.info()

# Taking a smaller sample data for initial analysis
df = df.sample(50000, random_state=1)
df.info()

"""# 2. Perform data pre-processing on the data:

"""

df = df.drop(['id', 'date'], axis=1)
df.head()

#df.reset_index(drop=True, inplace=True)
df.head()

"""## Data preprocessing:
Data cleansing by removing unwanted characters, spaces, stop words etc. Convert text to lowercase.
"""

import re
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud, STOPWORDS

#@jit(target='cuda')
def process_data(text):
  # Remove unwanted characters, only keeping alphabetic words
  alpha_text = re.sub(r'[^A-Za-z]+',' ', text )
  # Convert to lower text
  lower_text = alpha_text.lower()
  # Remove Stop words 
  stop_words = set(stopwords.words('english'))
  text_wo_sw = ' '.join([words for words in lower_text.split() if words not in stop_words])
  # Lemmetization
  lemma = WordNetLemmatizer()
  processed_text = [lemma.lemmatize(word) for word in text_wo_sw]
  processed_text = "".join(processed_text)
  return processed_text

# Check how the process_data function words on a smaller text
t = "Info has been found (+/- 100 pages,. urlLink urlLink 16-feb-04"
t1 = process_data(t)
print(t1)

# Once above step is successful, run apply the function on the full dataset
df['processed_text'] = df.text.apply(process_data)
df.head()

"""## Target/label merger and transformation

As there are multiple claseses gender, age, topic and sign, we will merge them into a single label 
"""

all_labels = df[['gender', 'age', 'topic', 'sign']]
all_labels.head()

all_labels.dtypes

# Age is int64, so lets convert it into string
all_labels['age'] = all_labels['age'].astype('str')
all_labels.dtypes

all_labels.shape

m = []
for i in range(all_labels.shape[0]):
  g = []
  for j in range(all_labels.shape[1]):
    g.append(all_labels.iloc[i][j])
  m.append(g)

df['labels'] = m

df.head()

# Create a new dataframe with processed_text and the labels only
new_df = df[['processed_text', 'labels']]
new_df.sample(5)

# Check for most frequent bloggers
new_df.labels.astype('str').value_counts()

# Check for null values
new_df.isna().sum()

"""## Train and test split
First separate the data in X and y and then do a train test split
"""

X = new_df['processed_text']
y = new_df['labels']

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, shuffle=True)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""#### Vectorization"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1,2), binary=True)

X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)
X_train_vect

vectorizer.get_feature_names()[:10]

print(X_train_vect[:3], vectorizer.get_feature_names()[:3])

# use the dataframe that we created earlier to create the dictionary of words and their counts
all_labels.head()

keys = []
values = []
for i in range(all_labels.shape[1]):
  col = all_labels.iloc[:, i].value_counts()
  for j in range(col.shape[0]):
    keys.append(col.index[j])
    values.append(col.iloc[j])

# Check a sample key value pair
i = 32
print(keys[i], values[i])

dictionary = dict(zip(keys, values))

"""### Convert the labels using MultiLabelBinarizer"""

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer(classes=sorted(dictionary.keys()))
y_train_mlb = mlb.fit_transform(y_train)
y_test_mlb = mlb.transform(y_test)

y_train_mlb[0]

y_test_mlb[0]

# To verify the binary label transormation lets check 1 value
y_train.iloc[1]

# And its inverse transformed value
mlb.inverse_transform(y_train_mlb)[1]

"""# 3. Design, train, tune and test the best text classifier"""

print(X_train_vect.shape, y_train_mlb.shape, X_test_vect.shape, y_test_mlb.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

logit = LogisticRegression(solver='lbfgs')
ovr = OneVsRestClassifier(logit)

with tf.device('/device:GPU:0'):
  ovr.fit(X_train_vect, y_train_mlb)
y_pred_logit = ovr.predict(X_test_vect)
y_pred_logit

"""# 4. Display and explain detail the classification report"""

from sklearn.metrics import classification_report
print(classification_report(y_test_mlb, y_pred_logit ))

"""# 5. Print the true vs predicted labels for any 5 entries from the dataset"""

#sample_pred = y_pred[:5]
sample_pred = y_pred_logit[:5]
actual_label = y_test_mlb[:5]

actual_label = mlb.inverse_transform(actual_label)

actual_label

sample_pred = mlb.inverse_transform(sample_pred)
sample_pred



"""# DOMAIN: Customer support
## CONTEXT: 
Great Learning has a an academic support department which receives numerous support requests every day throughout the year. Teams are spread across geographies and try to provide support round the year. Sometimes there are circumstances where due to heavy workload certain request resolutions are delayed, impacting company’s business. Some of the requests are very generic where a proper resolution procedure delivered to the user can solve the problem. Company is looking forward to design an automation which can
interact with the user, understand the problem and display the resolution procedure [ if found as a generic request ] or redirect the request
to an actual human support executive if the request is complex or not in it’s database.
## DATA DESCRIPTION: 
A sample corpus is attached for your reference. Please enhance/add more data to the corpus using your linguistics skills.
# PROJECT OBJECTIVE: 
Design a python based interactive semi - rule based chatbot which can do the following:
1. Start chat session with greetings and ask what the user is looking for.
2. Accept dynamic text based questions from the user. Reply back with relevant answer from the designed corpus.
3. End the chat session only if the user requests to end else ask what the user is looking for. Loop continues till the user asks to end it.

Please use the sample chatbot demo video for reference.

## EVALUATION: 
GL evaluator will use linguistics to twist and turn sentences to ask questions on the topics described in DATA DESCRIPTION and check if the bot is giving relevant replies.

### Hint: 
There are a lot of techniques using which one can clean and prepare the data which can be used to train a ML/DL classifier. Hence, it might require you to experiment, research, self learn and implement the above classifier. There might be many iterations between hand building the corpus and designing the best fit text classifier. As the quality and quantity of corpus increases the model’s performance i.e. ability to answer right questions also increases.

#### Reference: https://www.mygreatlearning.com/blog/basics-of-building-an-artificial-intelligence-chatbot/
"""

# Importing Corpus GL Bot
import json

with open(project_path + 'GL Bot.json') as file:
  corpus = json.load(file)

print(corpus)

import nltk
nltk.download('punkt')

# Extract data
# Tokens
W = []

# tags
L = []

#Tokenized words
X = []

#Tags
y = []

for intent in corpus['intents']:
  for pattern in intent['patterns']:
    w_tmp = nltk.word_tokenize(pattern)
    W.extend(w_tmp)
    X.append(w_tmp)
    y.append(intent['tag'])
  
  if (intent['tag'] not in L):
    L.append(intent['tag'])

print(W)

print(y)

# Stemming
stemmer = SnowballStemmer('english')
W = [stemmer.stem(w.lower()) for w in W if w != '?']
W = sorted(list(set(W)))
L = sorted(L)

L

X_train = []
y_train = []

empty = [0 for _ in range(len(L))]
empty

for i, doc in enumerate(X):
  bag = []
  w_tmp = [stemmer.stem(w.lower()) for w in doc]

  for w in W:
    if (w in w_tmp):
      bag.append(1)
    else:
      bag.append(0)
  out_row = empty[:]
  out_row[L.index(y[i])] = 1

  X_train.append(bag)
  y_train.append(out_row)

print(len(X_train), len(y_train))

len(y_train[1])

j = 2
print(X_train[j], '\n', y_train[j])

# Define a neural Network to train the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
model = Sequential()

model.add(Dense(64,  input_dim=(len(X_train[0])), activation='relu'))
model.add(Dropout(0.8))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.8))
model.add(Dense(len(y_train[0]), activation='softmax'))

optimizer = tf.keras.optimizers.Adam(lr=0.001, amsgrad=True )
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, batch_size=32, epochs=100)

for i in range(len(y_train)):
  if (len(y_train[i])> 8):
    print(len(y_train[i]))

for i in range(len(X_train)):
  if (len(X_train[i])> 154):
    print(len(X_train[i]))

def bow(input_text):
  bag = []
  input_tmp = nltk.word_tokenize(input_text)
  w_tmp = [stemmer.stem(w.lower()) for w in input_tmp]

  for w in W:
    if (w in w_tmp):
      bag.append(1)
    else:
      bag.append(0)
  out_row = empty[:]
  out_row[L.index(y[i])] = 1
  return bag

test = bow("Hi, how are you doing?")
print(test)
print(len(test))

# Text chat function
import random
def chat():
  print("Chat with Chatbot(type: stop to quit)")
  print('If you are not satisfied by the response: (type *)')
  while True:
    inp = input("\n\nYou: ")
    if (inp.lower() == "*"):
      print("BOT: Please rephrase your question and try again")
    if (inp.lower() == 'stop'):
      break
    
    #inp = [inp]
    print(inp)
    token = bow(inp)
    print(token)
    print(len(token))
    results = model.predict([token])
    results_index = np.argmax(results)
    tag = L[results_index]

    for tg in corpus['intents']:
      if (tg['tag'] == tag):
        responses = tg['responses']

    #print(random.choice(responses ))
    print(responses)

chat()

model.summary()

